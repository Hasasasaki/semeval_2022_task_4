{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hasasasaki/semeval_2022_task_4/blob/main/model_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4qHbOJcwcWcv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv_mP3i6UrPj",
        "outputId": "603db1d7-85e5-4546-e86b-4ba10fc23638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deep_translator in /usr/local/lib/python3.11/dist-packages (1.11.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "# !pip install \"flash_attn==2.6.3\" --no-build-isolation\n",
        "!pip install deep_translator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HglL0PQVLpR"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4n7udvqWVLpS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f780764-6141-4f8c-ff88-8abbbf722152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All random states have been reset with seed 42\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    TrainerCallback,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "\n",
        "seed = 42\n",
        "\n",
        "def reset_seeds(seed=seed):\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    import numpy as np\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "\n",
        "    print(f\"All random states have been reset with seed {seed}\")\n",
        "\n",
        "reset_seeds()\n",
        "\n",
        "new_columns = [\n",
        "    \"par_id\",      # 1 (integer ID)\n",
        "    \"art_id\",      # @@24942188 (article identifier)\n",
        "    \"topic\",       # hopeless (PCL category)\n",
        "    \"country\",     # ph (country code)\n",
        "    \"text\",        # Full text content\n",
        "    \"label\"        # 0 (binary label)\n",
        "]\n",
        "\n",
        "# Read main dataset - skip 4 disclaimer rows\n",
        "df = pd.read_csv(\n",
        "    \"data/dontpatronizeme_pcl.tsv\",\n",
        "    sep=\"\\t\",\n",
        "    header=None,\n",
        "    skiprows=4,\n",
        "    names=new_columns,\n",
        "    on_bad_lines='warn'\n",
        ")\n",
        "\n",
        "# Read train/dev splits\n",
        "train_val_labels = pd.read_csv(\"data/train_semeval_parids-labels.csv\")\n",
        "test_labels = pd.read_csv(\"data/dev_semeval_parids-labels.csv\")\n",
        "\n",
        "# Convert string labels to lists\n",
        "def parse_labels(label_str: str) -> list[int]:\n",
        "    return [int(x) for x in label_str.strip(\"[]\").replace(\" \", \"\").split(\",\")]\n",
        "\n",
        "# Process labels dataframes\n",
        "for labels_df in [train_val_labels, test_labels]:\n",
        "    labels_df['labels'] = labels_df['label'].apply(parse_labels)\n",
        "    labels_df.drop('label', axis=1, inplace=True)\n",
        "\n",
        "# Join with main data\n",
        "train_val_df = df.merge(train_val_labels, on=\"par_id\", how=\"inner\")\n",
        "test_df = df.merge(test_labels, on=\"par_id\", how=\"inner\")\n",
        "\n",
        "# Add PCL positivity column to both dataframes\n",
        "train_val_df['pcl_label'] = train_val_df['label'].apply(\n",
        "    lambda x: 0 if x in {0, 1} else 1)\n",
        "test_df['pcl_label'] = test_df['label'].apply(\n",
        "    lambda x: 0 if x in {0, 1} else 1)\n",
        "\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=0.2, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tlPG3JxjUGWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3e02f96-c121-4748-e170-1b1725fc3061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "# os.environ[\"TRANSFORMERS_CACHE\"] = \"/vol/bitbucket/bj321/.cache\"\n",
        "# nltk.data.path.append(\"/vol/bitbucket/bj321/nltk_data\")  # Your custom path\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSIrXlbcUGWp",
        "outputId": "15df43eb-4821-4cf2-e41b-03b36bc4a16e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2OzlLMoQUGWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdca4f87-b0ac-420e-bc0e-5ca70155e818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SentenceBERT model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "import concurrent.futures\n",
        "import time\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Load SentenceBERT model\n",
        "print(\"Loading SentenceBERT model...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # Smaller, faster model\n",
        "# Alternative: model = SentenceTransformer('paraphrase-mpnet-base-v2')  # More accurate but slower\n",
        "\n",
        "def compute_similarity(original, translated):\n",
        "    \"\"\"Compute cosine similarity between original and translated text embeddings\"\"\"\n",
        "    # Get embeddings\n",
        "    emb1 = model.encode([original])[0]\n",
        "    emb2 = model.encode([translated])[0]\n",
        "\n",
        "    # Compute cosine similarity (1 - cosine distance)\n",
        "    similarity = 1 - cosine(emb1, emb2)\n",
        "    return similarity\n",
        "\n",
        "def back_translate_single(item):\n",
        "    \"\"\"Process a single text item with similarity filtering\"\"\"\n",
        "    text, label, par_id, source, target, idx, similarity_threshold = item\n",
        "    try:\n",
        "        # First translation (source to target)\n",
        "        translated = GoogleTranslator(source=source, target=target).translate(text)\n",
        "        time.sleep(0.5)  # Avoid rate limiting\n",
        "\n",
        "        # Second translation (target back to source)\n",
        "        back_translated = GoogleTranslator(source=target, target=source).translate(translated)\n",
        "\n",
        "        # Compute semantic similarity\n",
        "        similarity = compute_similarity(text, back_translated)\n",
        "\n",
        "        # Only return translations that maintain semantic similarity\n",
        "        if similarity >= similarity_threshold:\n",
        "            return back_translated, label, par_id, idx, similarity, True\n",
        "        else:\n",
        "            print(f\"Low similarity ({similarity:.3f}) for item {idx}: discarded\")\n",
        "            return text, label, par_id, idx, similarity, False  # Return original text but mark as not augmented\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in item {idx}: {str(e)}\")\n",
        "        return None, None, None, idx, 0.0, False\n",
        "\n",
        "def back_translate_batch(texts, labels, par_ids, source='en', target='zh-CN', max_workers=5, similarity_threshold=0.75):\n",
        "    \"\"\"Process texts in parallel batches with similarity filtering\"\"\"\n",
        "    results = [None] * len(texts)\n",
        "    labels_out = [None] * len(labels)\n",
        "    par_ids_out = [None] * len(par_ids)\n",
        "    similarities = [0.0] * len(texts)\n",
        "    is_augmented = [False] * len(texts)\n",
        "\n",
        "    # Create work items\n",
        "    work_items = [(texts[i], labels[i], par_ids[i], source, target, i, similarity_threshold) for i in range(len(texts))]\n",
        "\n",
        "    # Process in parallel\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = [executor.submit(back_translate_single, item) for item in work_items]\n",
        "\n",
        "        # Process results as they complete\n",
        "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
        "            result, label, par_id, idx, similarity, augmented = future.result()\n",
        "            if result is not None:\n",
        "                results[idx] = result\n",
        "                labels_out[idx] = label\n",
        "                par_ids_out[idx] = par_id\n",
        "                similarities[idx] = similarity\n",
        "                is_augmented[idx] = augmented\n",
        "\n",
        "    # Create DataFrame with results\n",
        "    result_df = pd.DataFrame({\n",
        "        'par_id': par_ids_out,\n",
        "        'original_text': texts,\n",
        "        'text': results,\n",
        "        'pcl_label': [int(x) if x is not None else None for x in labels_out],\n",
        "        'similarity': similarities,\n",
        "        'is_augmented': is_augmented\n",
        "    })\n",
        "\n",
        "    # Filter out None values\n",
        "    result_df = result_df.dropna(subset=['text'])\n",
        "\n",
        "    return result_df\n",
        "\n",
        "# Main processing loop\n",
        "language_list = ['zh-CN', 'fr', 'ru']\n",
        "similarity_threshold = 0.75  # Adjust as needed\n",
        "\n",
        "for language in language_list:\n",
        "    output_file = f'data/backtrans_data_{language}.csv'\n",
        "\n",
        "    if not os.path.exists(output_file):\n",
        "        print(f\"Processing language: {language}\")\n",
        "\n",
        "        # Process in chunks to avoid memory issues\n",
        "        chunk_size = 100\n",
        "        all_results_df = pd.DataFrame()\n",
        "\n",
        "        for i in range(0, len(train_df), chunk_size):\n",
        "            chunk_texts = train_df['text'].iloc[i:i+chunk_size].tolist()\n",
        "            chunk_labels = train_df['pcl_label'].iloc[i:i+chunk_size].tolist()\n",
        "            chunk_par_ids = train_df['par_id'].iloc[i:i+chunk_size].tolist()\n",
        "\n",
        "            print(f\"Processing chunk {i//chunk_size + 1}/{len(train_df)//chunk_size + 1}\")\n",
        "            result_df = back_translate_batch(\n",
        "                chunk_texts,\n",
        "                chunk_labels,\n",
        "                chunk_par_ids,\n",
        "                source='en',\n",
        "                target=language,\n",
        "                max_workers=5,\n",
        "                similarity_threshold=similarity_threshold\n",
        "            )\n",
        "\n",
        "            all_results_df = pd.concat([all_results_df, result_df])\n",
        "\n",
        "            # Save intermediate results\n",
        "            all_results_df.to_csv(f'data/backtrans_temp_{language}.csv', index=False)\n",
        "\n",
        "            # Optional: Add a delay between chunks\n",
        "            time.sleep(2)\n",
        "\n",
        "        # Save final results\n",
        "        all_results_df.to_csv(output_file, index=False)\n",
        "\n",
        "        # Print statistics\n",
        "        total = len(all_results_df)\n",
        "        augmented = all_results_df['is_augmented'].sum()\n",
        "        print(f\"Completed {language}: {total} samples processed\")\n",
        "        print(f\"Kept {augmented} samples ({augmented/total:.1%}) with similarity ≥ {similarity_threshold}\")\n",
        "        print(f\"Average similarity: {all_results_df['similarity'].mean():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCa0qvJqVLpT"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-mKaqTS0VLpT"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "lr = 1e-5\n",
        "n_epochs = 2\n",
        "betas = (0.9, 0.98)\n",
        "eps = 1e-6\n",
        "wd = 1e-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0oXKJCEbVLpT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f337a4-85d1-441c-c9d1-753576e99b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data/backtrans_data_de.csv with 10468 rows\n",
            "Loaded data/backtrans_data_fr.csv with 10468 rows\n",
            "Loaded data/backtrans_data_zh-CN.csv with 10468 rows\n",
            "Combined backtranslation data: 20100 rows\n",
            "   par_id      art_id     topic country  \\\n",
            "0    5825   @@9438566   in-need      au   \n",
            "1    7171   @@1934487  hopeless      ng   \n",
            "2     680   @@9525972   in-need      nz   \n",
            "3    4906  @@22596758   refugee      bd   \n",
            "4    8180  @@13717053   migrant      ph   \n",
            "\n",
            "                                                text label labels  pcl_label  \n",
            "0  It described the local police as under resourc...  None   None          0  \n",
            "1  The only force that is able to stop it is the ...  None   None          1  \n",
            "2  The government's plans to return to mass finan...  None   None          0  \n",
            "3  New figures show that more than 48,000 Rohingy...  None   None          0  \n",
            "4  He then listed several immigrants, mainly from...  None   None          0  \n"
          ]
        }
      ],
      "source": [
        "class PCLDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, balance_method='oversample', seed=seed):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "         # Split into positive and negative classes\n",
        "        pos_df = dataframe[dataframe['pcl_label'] == 1]\n",
        "        neg_df = dataframe[dataframe['pcl_label'] == 0]\n",
        "\n",
        "        # Balance classes\n",
        "        if balance_method == 'oversample':\n",
        "            # Repeat minority class samples\n",
        "            if len(pos_df) > len(neg_df):\n",
        "                pos_df, neg_df = neg_df, pos_df\n",
        "            n_samples = max(len(pos_df), len(neg_df))\n",
        "            pos_df = pos_df.sample(n_samples, replace=True, random_state=seed)\n",
        "        elif balance_method == 'undersample':\n",
        "            # Take minimum number of samples\n",
        "            n_samples = min(len(pos_df), len(neg_df))\n",
        "            pos_df = pos_df.sample(n_samples, random_state=seed)\n",
        "            neg_df = neg_df.sample(n_samples, random_state=seed)\n",
        "        elif balance_method == 'None':\n",
        "            pass\n",
        "\n",
        "        # Combine and shuffle\n",
        "        balanced_df = pd.concat([pos_df, neg_df]).sample(frac=1, random_state=seed)\n",
        "        self.texts = balanced_df['text'].tolist()\n",
        "        self.labels = balanced_df['pcl_label'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Initialize tokenizer and datasets\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
        "\n",
        "# Load all backtranslation files and combine them\n",
        "backtrans_files = [\n",
        "    'data/backtrans_data_de.csv',\n",
        "    # 'data/backtrans_data_es.csv',\n",
        "    'data/backtrans_data_fr.csv',\n",
        "    # 'data/backtrans_data_ru.csv',\n",
        "    'data/backtrans_data_zh-CN.csv'\n",
        "]\n",
        "\n",
        "backtrans_dfs = []\n",
        "for file in backtrans_files:\n",
        "    try:\n",
        "        cur_df = pd.read_csv(file)\n",
        "        backtrans_dfs.append(cur_df)\n",
        "        print(f\"Loaded {file} with {len(df)} rows\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "# Combine all backtranslation dataframes\n",
        "if backtrans_dfs:\n",
        "    backtrans_df = pd.concat(backtrans_dfs, ignore_index=True)\n",
        "    print(f\"Combined backtranslation data: {len(backtrans_df)} rows\")\n",
        "else:\n",
        "    backtrans_df = pd.DataFrame()\n",
        "    print(\"No backtranslation data found\")\n",
        "\n",
        "# Create datasets\n",
        "for col in train_df.columns:\n",
        "    if col not in backtrans_df.columns:\n",
        "        backtrans_df[col] = None\n",
        "\n",
        "backtrans_df = backtrans_df[train_df.columns]\n",
        "backtrans_df['pcl_label'] = backtrans_df['pcl_label'].astype(int)\n",
        "augmented_train_df = pd.concat([backtrans_df, train_df], ignore_index=True)\n",
        "print(augmented_train_df.head())\n",
        "train_dataset = PCLDataset(augmented_train_df, tokenizer)\n",
        "val_dataset = PCLDataset(val_df, tokenizer, balance_method='None')\n",
        "test_dataset = PCLDataset(test_df, tokenizer, balance_method='None')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ovyIzdKUGWq"
      },
      "source": [
        "## Weighted Random Sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cozqcQtdUGWq"
      },
      "outputs": [],
      "source": [
        "class WeightedRandomSamplerTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.weights = torch.FloatTensor(self._get_weights())\n",
        "        self.sampler = WeightedRandomSampler(self.weights, len(self.weights), replacement=True)\n",
        "\n",
        "    def get_train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.args.train_batch_size, sampler=self.sampler, collate_fn=self.data_collator)\n",
        "\n",
        "    def _get_weights(self):\n",
        "        labels = np.array(self.train_dataset.labels)\n",
        "        class_counts = np.bincount(labels)\n",
        "        class_weights = 1.0 / np.sqrt(class_counts.astype(np.float32))\n",
        "        weights = class_weights[labels]\n",
        "        return weights\n",
        "\n",
        "train_dataset = PCLDataset(augmented_train_df, tokenizer, 'None')\n",
        "val_dataset = PCLDataset(val_df, tokenizer, balance_method='None')\n",
        "test_dataset = PCLDataset(test_df, tokenizer, balance_method='None')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aBn4QsCRVLpT"
      },
      "outputs": [],
      "source": [
        "# reset_seeds()\n",
        "# model_config = AutoConfig.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
        "# model_config.mlp_dropout = 0.2\n",
        "# model_config.num_labels = 2\n",
        "\n",
        "# # Initialize model with classification head\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#     \"answerdotai/ModernBERT-base\",\n",
        "#     num_labels=2,\n",
        "# )\n",
        "# model.train()\n",
        "# Training setup\n",
        "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(\"Using device:\", device)\n",
        "# model.to(device)\n",
        "# training_args = TrainingArguments(\n",
        "#     seed=seed,\n",
        "#     data_seed=seed,\n",
        "#     dataloader_num_workers=0,\n",
        "#     output_dir=f\"ModernBERT_pcl_ft\",\n",
        "#     learning_rate=lr,\n",
        "#     per_device_train_batch_size=batch_size,\n",
        "#     per_device_eval_batch_size=batch_size,\n",
        "#     num_train_epochs=n_epochs,\n",
        "#     lr_scheduler_type=\"cosine\",\n",
        "#     optim=\"adamw_torch_fused\",\n",
        "#     adam_beta1=betas[0],\n",
        "#     adam_beta2=betas[1],\n",
        "#     adam_epsilon=eps,\n",
        "#     # weight_decay=wd,\n",
        "#     logging_strategy=\"epoch\",\n",
        "#     eval_strategy=\"epoch\",\n",
        "#     save_strategy=\"epoch\",\n",
        "#     load_best_model_at_end=True,\n",
        "#     bf16=True,\n",
        "#     bf16_full_eval=True,\n",
        "#     push_to_hub=False,\n",
        "#     warmup_ratio=0.1,\n",
        "#     full_determinism=True\n",
        "\n",
        "# )\n",
        "\n",
        "def clean_memory(force_gc=3):\n",
        "    \"\"\"\n",
        "    Thoroughly clean GPU memory by moving tensors to CPU first\n",
        "\n",
        "    Args:\n",
        "        force_gc: Number of garbage collection passes to make\n",
        "    \"\"\"\n",
        "    # Move any remaining tensors to CPU\n",
        "    for obj in gc.get_objects():\n",
        "        try:\n",
        "            if torch.is_tensor(obj) and obj.is_cuda:\n",
        "                obj.cpu()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Clear PyTorch cache\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Multiple garbage collection passes\n",
        "    for _ in range(force_gc):\n",
        "        gc.collect()\n",
        "\n",
        "    # Extra memory cleanup for CUDA\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Add a small delay to allow memory to be properly freed\n",
        "    time.sleep(1)\n",
        "\n",
        "    print(\"Memory cleaned\")\n",
        "\n",
        "def safe_cleanup(trainer=None, model=None):\n",
        "    \"\"\"Safely clean up trainer and model to free GPU memory\"\"\"\n",
        "    if trainer is not None:\n",
        "        # Remove model reference from trainer to avoid double deletion\n",
        "        if hasattr(trainer, 'model'):\n",
        "            if hasattr(trainer.model, 'to'):\n",
        "                trainer.model.to('cpu')\n",
        "            trainer.model = None\n",
        "\n",
        "        # Clear any optimizer states that might be on GPU\n",
        "        if hasattr(trainer, 'optimizer'):\n",
        "            trainer.optimizer = None\n",
        "\n",
        "        # Clear any scheduler states\n",
        "        if hasattr(trainer, 'lr_scheduler'):\n",
        "            trainer.lr_scheduler = None\n",
        "\n",
        "        # Delete the trainer\n",
        "        del trainer\n",
        "\n",
        "    # Move model to CPU and delete\n",
        "    if model is not None:\n",
        "        if hasattr(model, 'to'):\n",
        "            model.to('cpu')\n",
        "        del model\n",
        "\n",
        "    # Run garbage collection\n",
        "    clean_memory()\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Calculate classification metrics for Hugging Face Trainer\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average='binary'\n",
        "    )\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# class MetricsCallback(TrainerCallback):\n",
        "#     def __init__(self):\n",
        "#         self.training_history = {\"train\": [], \"eval\": []}\n",
        "\n",
        "#     def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "#         if logs is not None:\n",
        "#             if \"loss\" in logs:  # Training logs\n",
        "#                 self.training_history[\"train\"].append(logs)\n",
        "#             elif \"eval_loss\" in logs:  # Evaluation logs\n",
        "#                 self.training_history[\"eval\"].append(logs)\n",
        "\n",
        "\n",
        "\n",
        "class MetricsCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.training_history = {\"train\": [], \"eval\": []}\n",
        "        self.best_f1 = 0.0\n",
        "        self.best_epoch = 0\n",
        "        self.best_step = 0\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
        "        self.training_history[\"eval\"].append(metrics)\n",
        "        # Track best F1 score\n",
        "        if metrics.get(\"eval_f1\", 0) > self.best_f1:\n",
        "            self.best_f1 = metrics.get(\"eval_f1\", 0)\n",
        "            self.best_epoch = state.epoch\n",
        "            self.best_step = state.global_step\n",
        "\n",
        "    def on_log(self, args, state, control, logs, **kwargs):\n",
        "        if \"loss\" in logs:\n",
        "            self.training_history[\"train\"].append(logs)\n",
        "\n",
        "\n",
        "\n",
        "def train_model_with_seed(seed, model_name, train_dataset, val_dataset, output_dir):\n",
        "    reset_seeds(seed)\n",
        "\n",
        "    # Create a unique output directory for this seed\n",
        "    seed_output_dir = f\"{output_dir}/seed_{seed}\"\n",
        "    os.makedirs(seed_output_dir, exist_ok=True)\n",
        "\n",
        "    # Load model with seed-specific configuration\n",
        "    config = AutoConfig.from_pretrained(model_name, num_labels=2)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "      seed=seed,\n",
        "      data_seed=seed,\n",
        "      dataloader_num_workers=0,\n",
        "      output_dir=seed_output_dir,\n",
        "      learning_rate=lr,\n",
        "      per_device_train_batch_size=batch_size,\n",
        "      per_device_eval_batch_size=batch_size,\n",
        "      num_train_epochs=n_epochs,\n",
        "      lr_scheduler_type=\"cosine\",\n",
        "      optim=\"adamw_torch_fused\",\n",
        "      adam_beta1=betas[0],\n",
        "      adam_beta2=betas[1],\n",
        "      adam_epsilon=eps,\n",
        "      # weight_decay=wd,\n",
        "      logging_strategy=\"epoch\",\n",
        "      eval_strategy=\"epoch\",\n",
        "      save_strategy=\"epoch\",\n",
        "      load_best_model_at_end=True,\n",
        "      bf16=True,\n",
        "      bf16_full_eval=True,\n",
        "      push_to_hub=False,\n",
        "      warmup_ratio=0.1,\n",
        "      # full_determinism=True\n",
        "    )\n",
        "\n",
        "    # Initialize metrics callback\n",
        "    metrics_callback = MetricsCallback()\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        processing_class=tokenizer,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[metrics_callback],\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(f\"Training model with seed {seed}...\")\n",
        "    trainer.train()\n",
        "\n",
        "    clean_memory()\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    # Save the best model path and its metrics\n",
        "    best_checkpoint = os.path.join(seed_output_dir, f\"checkpoint-{metrics_callback.best_step}\")\n",
        "\n",
        "    result = {\n",
        "        \"seed\": seed,\n",
        "        \"f1_score\": metrics_callback.best_f1,\n",
        "        \"best_checkpoint\": best_checkpoint,\n",
        "        \"eval_results\": eval_results,\n",
        "        \"best_epoch\": metrics_callback.best_epoch\n",
        "    }\n",
        "\n",
        "    # Delete model and trainer to free memory\n",
        "    print('before clean')\n",
        "    safe_cleanup(trainer, model)\n",
        "    print(\"after clean\")\n",
        "    return result\n",
        "\n",
        "\n",
        "def predict_with_model(model_path, test_dataset, tokenizer=None):\n",
        "    \"\"\"Use Trainer.predict() to get predictions from a model checkpoint\"\"\"\n",
        "    # Load model and tokenizer from checkpoint\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "    if tokenizer is None:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "    # Create a temporary trainer for prediction\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./tmp_predict\",\n",
        "        per_device_eval_batch_size=32,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        processing_class=tokenizer,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Get predictions\n",
        "    print(f\"Getting predictions from model at {model_path}\")\n",
        "    predictions = trainer.predict(test_dataset)\n",
        "\n",
        "    # Extract logits and convert to probabilities\n",
        "    logits = predictions.predictions\n",
        "    probs = F.softmax(torch.tensor(logits), dim=1).numpy()\n",
        "\n",
        "    # Get class predictions (0 or 1)\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "\n",
        "    return preds, probs\n",
        "\n",
        "def ensemble_predict(model_paths, test_dataset, tokenizer=None):\n",
        "    \"\"\"Combine predictions from multiple models using majority voting\"\"\"\n",
        "    all_predictions = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    # Get predictions from each model\n",
        "    for model_path in model_paths:\n",
        "        preds, probs = predict_with_model(model_path, test_dataset, tokenizer)\n",
        "        all_predictions.append(preds)\n",
        "        all_probabilities.append(probs)\n",
        "\n",
        "    # Stack predictions for voting\n",
        "    stacked_preds = np.stack(all_predictions)\n",
        "\n",
        "    # Majority voting (mode of predictions)\n",
        "    voted_predictions = []\n",
        "    for i in range(len(test_dataset)):\n",
        "        votes = stacked_preds[:, i]\n",
        "        # Find most common prediction (0 or 1)\n",
        "        voted_pred = Counter(votes).most_common(1)[0][0]\n",
        "        voted_predictions.append(voted_pred)\n",
        "\n",
        "    # Average probabilities\n",
        "    avg_probs = np.mean(np.stack(all_probabilities), axis=0)\n",
        "\n",
        "    return np.array(voted_predictions), avg_probs\n",
        "\n",
        "def train_ensemble(model_name, train_dataset, val_dataset, test_dataset, num_models=10, output_dir=f\"ModernBERT_pcl_ft\"):\n",
        "    \"\"\"Train multiple models with different seeds and create an ensemble\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Train multiple models with different seeds\n",
        "    model_results = []\n",
        "    seeds = list(range(42, 42 + num_models))  # Use seeds 42 to 51\n",
        "\n",
        "    for seed in seeds:\n",
        "        result = train_model_with_seed(seed, model_name, train_dataset, val_dataset, output_dir)\n",
        "        model_results.append(result)\n",
        "        print(f\"Seed {seed} - F1 Score: {result['f1_score']:.4f}\")\n",
        "        clean_memory()\n",
        "\n",
        "    # Sort models by validation F1 score and select top 3\n",
        "    model_results.sort(key=lambda x: x['f1_score'], reverse=True)\n",
        "    top_models = model_results[:3]\n",
        "\n",
        "    print(\"\\nTop 3 models:\")\n",
        "    for i, model in enumerate(top_models):\n",
        "        print(f\"{i+1}. Seed {model['seed']} - F1 Score: {model['f1_score']:.4f}\")\n",
        "\n",
        "    # Get the checkpoint paths for the top models\n",
        "    top_model_paths = [model[\"best_checkpoint\"] for model in top_models]\n",
        "\n",
        "    # Save ensemble metadata\n",
        "    ensemble_info = {\n",
        "        \"models\": [{\"seed\": m[\"seed\"], \"checkpoint\": m[\"best_checkpoint\"], \"f1_score\": m[\"f1_score\"]} for m in top_models],\n",
        "        \"creation_date\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "\n",
        "    pd.DataFrame(ensemble_info[\"models\"]).to_csv(f\"{output_dir}/ensemble_models.csv\", index=False)\n",
        "\n",
        "    # Evaluate ensemble on test set\n",
        "    print(\"\\nEvaluating ensemble on test set...\")\n",
        "    ensemble_preds, ensemble_probs = ensemble_predict(top_model_paths, test_dataset)\n",
        "\n",
        "    # Get true labels from test dataset\n",
        "    true_labels = [item['labels'].item() for item in test_dataset]\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(true_labels, ensemble_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, ensemble_preds, average='binary')\n",
        "\n",
        "    print(f\"Ensemble Test Results:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "    # Compare with individual model performance\n",
        "    print(\"\\nComparing with individual model performance:\")\n",
        "    for i, model_path in enumerate(top_model_paths):\n",
        "        model_preds, _ = predict_with_model(model_path, test_dataset)\n",
        "        model_accuracy = accuracy_score(true_labels, model_preds)\n",
        "        model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(true_labels, model_preds, average='binary')\n",
        "\n",
        "        print(f\"Model {i+1} (Seed {top_models[i]['seed']}):\")\n",
        "        print(f\"  Accuracy: {model_accuracy:.4f}\")\n",
        "        print(f\"  F1 Score: {model_f1:.4f}\")\n",
        "        print(f\"  Precision: {model_precision:.4f}\")\n",
        "        print(f\"  Recall: {model_recall:.4f}\")\n",
        "\n",
        "    # Create a prediction function for future use\n",
        "    def predict_with_ensemble(new_dataset):\n",
        "        \"\"\"Function to make predictions with the ensemble on new data\"\"\"\n",
        "        return ensemble_predict(top_model_paths, new_dataset)\n",
        "\n",
        "    return top_model_paths, ensemble_preds, ensemble_probs, predict_with_ensemble\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# trainer = WeightedRandomSamplerTrainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=val_dataset,\n",
        "#     processing_class=tokenizer,\n",
        "#     data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "#     compute_metrics=compute_metrics,\n",
        "# )\n",
        "\n",
        "# metrics_callback = MetricsCallback()\n",
        "# trainer.add_callback(metrics_callback)\n",
        "\n",
        "# trainer.train()\n",
        "\n",
        "# train_history_df = pd.DataFrame(metrics_callback.training_history[\"train\"])\n",
        "# train_history_df = train_history_df.add_prefix(\"train_\")\n",
        "# eval_history_df = pd.DataFrame(metrics_callback.training_history[\"eval\"])\n",
        "# train_res_df = pd.concat([train_history_df, eval_history_df], axis=1)\n",
        "\n",
        "# args_df = pd.DataFrame([training_args.to_dict()])\n",
        "\n",
        "# display(train_res_df)\n",
        "# display(args_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your model name\n",
        "model_name = \"answerdotai/ModernBERT-base\"  # or your preferred model\n",
        "clean_memory()\n",
        "# Train the ensemble\n",
        "top_model_paths, ensemble_preds, ensemble_probs, predict_fn = train_ensemble(\n",
        "    model_name=model_name,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    test_dataset=test_dataset,\n",
        "    num_models=10,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "se4k2-CqlsZu",
        "outputId": "7c916839-1122-4013-e283-eb5a2128c01c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/__init__.py:1021: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
            "  return isinstance(obj, torch.Tensor)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory cleaned\n",
            "All random states have been reset with seed 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with seed 42...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjiangby03\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250301_214215-go3aew0m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jiangby03/huggingface/runs/go3aew0m' target=\"_blank\">ModernBERT_pcl_ft/seed_42</a></strong> to <a href='https://wandb.ai/jiangby03/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jiangby03/huggingface' target=\"_blank\">https://wandb.ai/jiangby03/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jiangby03/huggingface/runs/go3aew0m' target=\"_blank\">https://wandb.ai/jiangby03/huggingface/runs/go3aew0m</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3350' max='3350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3350/3350 07:21, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.232800</td>\n",
              "      <td>0.214350</td>\n",
              "      <td>0.921791</td>\n",
              "      <td>0.576052</td>\n",
              "      <td>0.635714</td>\n",
              "      <td>0.526627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.084000</td>\n",
              "      <td>0.364406</td>\n",
              "      <td>0.927761</td>\n",
              "      <td>0.584192</td>\n",
              "      <td>0.696721</td>\n",
              "      <td>0.502959</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/__init__.py:1021: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
            "  return isinstance(obj, torch.Tensor)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory cleaned\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InternalTorchDynamoError",
          "evalue": "RuntimeError: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalTorchDynamoError\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-bfb776a9f3cf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclean_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train the ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m top_model_paths, ensemble_preds, ensemble_probs, predict_fn = train_ensemble(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-6a0fb385c40d>\u001b[0m in \u001b[0;36mtrain_ensemble\u001b[0;34m(model_name, train_dataset, val_dataset, test_dataset, num_models, output_dir)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_with_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0mmodel_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Seed {seed} - F1 Score: {result['f1_score']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-6a0fb385c40d>\u001b[0m in \u001b[0;36mtrain_model_with_seed\u001b[0;34m(seed, model_name, train_dataset, val_dataset, output_dir)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;31m# Evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;31m# Save the best model path and its metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4072\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4073\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4074\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4075\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4266\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4267\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4268\u001b[0m             \u001b[0mmain_input_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"main_input_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4269\u001b[0m             inputs_decode = (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4481\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mloss_without_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4482\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4483\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4484\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3729\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3730\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3731\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3732\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3733\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/modernbert/modeling_modernbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, sliding_window_mask, position_ids, inputs_embeds, labels, indices, cu_seqlens, max_seqlen, batch_size, seq_len, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_set_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1240\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/modernbert/modeling_modernbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, sliding_window_mask, position_ids, inputs_embeds, indices, cu_seqlens, max_seqlen, batch_size, seq_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    974\u001b[0m                 )\n\u001b[1;32m    975\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                 layer_outputs = encoder_layer(\n\u001b[0m\u001b[1;32m    977\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/modernbert/modeling_modernbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, sliding_window_mask, position_ids, cu_seqlens, max_seqlen, output_attentions)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         mlp_output = (\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreference_compile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0;31m# Restore the dynamic layer stack depth if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompile_lock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_current_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0;31m# skip=1: skip this frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m             return self._torchdynamo_orig_callable(\n\u001b[0m\u001b[1;32m   1270\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_entry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0mcounters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"frames\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             result = self._inner_convert(\n\u001b[0m\u001b[1;32m   1065\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_entry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         return _compile(\n\u001b[0m\u001b[1;32m    527\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m                 \u001b[0;31m# Rewrap for clarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m                 raise InternalTorchDynamoError(\n\u001b[0m\u001b[1;32m    953\u001b[0m                     \u001b[0;34mf\"{type(e).__qualname__}: {str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                 ).with_traceback(e.__traceback__) from None\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0mguarded_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0mguarded_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mguarded_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mcompile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdynamo_timed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_compile.compile_inner\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"entire_frame_compile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCompileTimeInstructionCounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_compile_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcompile_time_strobelight_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"compile_inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils_internal.py\u001b[0m in \u001b[0;36mwrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mStrobelightCompileTimeProfiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             return StrobelightCompileTimeProfiler.profile_compile_time(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0mCompileContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattempt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                 \u001b[0mout_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_code_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRestartAnalysis\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/bytecode_transformation.py\u001b[0m in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1320\u001b[0m     \u001b[0mpropagate_line_nums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m     \u001b[0mtransformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclean_and_assemble_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rng_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_rng_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcuda_rng_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rng_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda_rng_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_cublas_allow_tf32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallow_tf32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_from_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprior_fwd_from_src\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mset_rng_state\u001b[0;34m(new_state, device)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mcb\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mdefault_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_generators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalTorchDynamoError\u001b[0m: RuntimeError: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/file.zip /content/ModernBERT_pcl_ft\n",
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ],
      "metadata": {
        "id": "R3HRdZVaYSw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKS9Ma6aVLpT"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO_h-VE7VLpT",
        "outputId": "c0ca6219-1156-4787-e9da-a86820af5186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   par_id      art_id      topic country  \\\n",
            "0       1  @@24942188   hopeless      ph   \n",
            "1       2  @@21968160    migrant      gh   \n",
            "2       3  @@16584954  immigrant      ie   \n",
            "3       4   @@7811231   disabled      nz   \n",
            "4       5   @@1494111    refugee      ca   \n",
            "\n",
            "                                                text  label  \n",
            "0  We 're living in times of absolute insanity , ...      0  \n",
            "1  In Libya today , there are countless number of...      0  \n",
            "2  White House press secretary Sean Spicer said t...      0  \n",
            "3  Council customers only signs would be displaye...      0  \n",
            "4  \" Just like we received migrants fleeing El Sa...      0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-90-ff13329d1276>:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(logits).cpu().numpy()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00977466, 0.9902254 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "checkpoint_path = \"./ModernBERT_pcl_ft/checkpoint-1676\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
        "\n",
        "\n",
        "# Evaluation on a single example\n",
        "def predict_single(text: str, model, tokenizer, device='cuda'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        max_length=512,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    return F.softmax(logits).cpu().numpy()\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "test_input = df[df['label'] == 3]['text'].iloc[3]\n",
        "predict_single(test_input, model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eizio0pSVLpU"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7IGhWHPyjpU"
      },
      "outputs": [],
      "source": [
        "# model_name = \"Hasasasaki/modernBERT_pcl_ft\"\n",
        "# model.push_to_hub(model_name)\n",
        "# tokenizer.push_to_hub(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "riUan1skzH_P",
        "outputId": "fc2b2f8a-d246-45bf-b0a2-74b9b2b5f96c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='131' max='131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [131/131 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.34968075156211853,\n",
              " 'eval_model_preparation_time': 0.0058,\n",
              " 'eval_accuracy': 0.9144768275203058,\n",
              " 'eval_f1': 0.535064935064935,\n",
              " 'eval_precision': 0.553763440860215,\n",
              " 'eval_recall': 0.5175879396984925,\n",
              " 'eval_runtime': 7.1778,\n",
              " 'eval_samples_per_second': 291.592,\n",
              " 'eval_steps_per_second': 18.251}"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.evaluate(test_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}